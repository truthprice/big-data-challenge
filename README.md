# big-data-challenge

## Description

The purpose and motivation of this project was to perform an extract, transform and load process on large, publically available data sets. Due to the size of the data sets, cloud based services such as google's colab and Amazon RDS. Given the size of the data sources, two notebooks were created with all extracted and transformed data being loaded into RDS databases. Using these cloud based services allowed us to process the data faster and relieve our machines of storage demands. 

## Usage

The two notebooks Level1a (2) and Level1b should be loaded into the google colab environment. Ensure that the proper Spark version is being used. Once all extracting and transforming is complete, ensure that Amazon RDS databases are created to load the dataframes into tables. The RDS endpoint and password have been removed from the files. Please use your own database names, endpoints, usernames and passwords. 
